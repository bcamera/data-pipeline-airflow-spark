# ğŸš€ Data Pipeline com Airflow + Spark + S3

Este projeto demonstra a construÃ§Ã£o de um pipeline de dados moderno utilizando **Apache Airflow** para orquestraÃ§Ã£o, **Apache Spark** (**PySpark**) para processamento distribuÃ­do e **Amazon S3** (ou MinIO em ambiente local) como Data Lake. O objetivo Ã© implementar um fluxo **ETL (Extract, Transform, Load)** completo, organizando os dados em camadas de um Data Lake (Raw â†’ Refined â†’ Curated).

-----

### ğŸ“Œ Arquitetura do Projeto

```mermaid
flowchart LR
Â  Â  A[API PÃºblica / Dataset] -->|Extract| B[Airflow]
Â  Â  B -->|Load Raw Data| C[S3 - Raw Layer]
Â  Â  B -->|Trigger| D[Spark Job]
Â  Â  D -->|Transform| E[S3 - Refined Layer]
Â  Â  E -->|Aggregate & Clean| F[S3 - Curated Layer]

```

### âš™ï¸ Tecnologias Utilizadas

  * Python 3.10+
  * Apache Airflow
  * Apache Spark / PySpark
  * Amazon S3
  * Docker + Docker Compose

-----

### ğŸ“‚ Estrutura do RepositÃ³rio


ğŸ“‚ data-pipeline-airflow-spark
 â”£ ğŸ“‚ dags              # DAGs do Airflow
 â”£ ğŸ“‚ spark_jobs        # Scripts PySpark
 â”£ ğŸ“‚ data              # Exemplos de dados
 â”£ ğŸ“‚ docs              # Diagramas e prints
 â”£ docker-compose.yml   # Subir ambiente Airflow + Spark + MinIO
 â”£ requirements.txt     # DependÃªncias
 â”— README.md            # Este arquivo
```

-----

### â–¶ï¸ Como Executar

#### **1. Clonar o repositÃ³rio**

```bash
git clone https://github.com/bcamera/data-pipeline-airflow-spark.git
cd data-pipeline-airflow-spark
```

#### **2. Subir os containers**

```bash
docker-compose up -d
```

#### **3. Acessar o Airflow**

  * **UI:** http://localhost:8080
  * **User:** `airflow` | **Password:** `airflow`

#### **4. Executar a DAG**

Habilite a DAG `etl_pipeline` na interface do Airflow.

-----

### ğŸ“Š Exemplo de Fluxo ETL

  * **Extract:** Coleta dados de exemplo (ex.: COVID-19 Data ou Yahoo Finance).
  * **Load Raw:** Salva os dados brutos no bucket `raw/`.
  * **Transform (Spark):** Limpeza e agregaÃ§Ã£o.
  * **Load Curated:** Salva dados prontos para consumo em `curated/`.

-----

### ğŸ“Œ PrÃ³ximos Passos

  * Implementar DAG inicial de ingestÃ£o.
  * Criar job PySpark de transformaÃ§Ã£o.
  * Integrar com S3 (ou MinIO local).
  * Adicionar monitoramento de falhas no Airflow.

-----

### âœ¨ Autor

  * ğŸ‘¨â€ğŸ’» Bruno Camera
  * ğŸ’¡ Analista de Banco de Dados em transiÃ§Ã£o para Engenharia de Dados & IA
